{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import operator\n",
    "import numpy as np\n",
    "import functools\n",
    "from sklearn.tree import DecisionTreeRegressor as DTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\xzou\\\\Downloads\\\\dataSets\\\\training\\\\KMSP_MTI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_train_data(time_granularity):\n",
    "    all_data = pd.read_csv(\"volume(table 6)_training.csv\")\n",
    "    start_datetime = datetime(2016,9,19,0,0,0)\n",
    "    tid_direction_dict = {}\n",
    "    for tid in [1,2,3]:\n",
    "        for direction in [0,1]:\n",
    "            tid_direction_data = all_data[(all_data.tollgate_id == tid) & (all_data.direction == direction)] \\\n",
    "                                .drop([\"tollgate_id\", \"direction\"], axis = 1)\n",
    "            if tid_direction_data.shape[0] > 0:\n",
    "                tid_direction_dict[(tid, direction)] = tid_direction_data.sort([\"time\"])\n",
    "    for key, tid_direction_df in tid_direction_dict.iteritems():\n",
    "        time_list = tid_direction_df[\"time\"].tolist()\n",
    "        last_pos = 0\n",
    "        pos = 0\n",
    "        time_window = []\n",
    "        missing_timewindow = []\n",
    "        missing_records_dict = {}\n",
    "        current_datetime = start_datetime\n",
    "        while pos < tid_direction_df.shape[0]:\n",
    "            this_time_interval_end = current_datetime + timedelta(minutes = time_granularity)\n",
    "            this_datetime_str = time_list[pos]\n",
    "            this_ymd = this_datetime_str.split(\" \")[0]\n",
    "            this_hms = this_datetime_str.split(\" \")[1]\n",
    "            int_this_ymd = map(int, this_ymd.split(\"-\"))\n",
    "            int_this_hms = map(int, this_hms.split(\":\"))\n",
    "            this_datetime = datetime(int_this_ymd[0], int_this_ymd[1], int_this_ymd[2], \n",
    "                                     int_this_hms[0], int_this_hms[1], int_this_hms[2])\n",
    "            if this_datetime >= current_datetime and this_datetime < this_time_interval_end:\n",
    "                time_window.append(get_time_window(current_datetime))\n",
    "                pos += 1\n",
    "            else:\n",
    "                if last_pos == pos: # insert a blank record representing the missing time window\n",
    "                    missing_timewindow.append(get_time_window(current_datetime))\n",
    "                else:\n",
    "                    last_pos = pos\n",
    "                current_datetime = this_time_interval_end\n",
    "\n",
    "        tid_direction_df[\"time_window\"] = time_window\n",
    "        tid_direction_df[\"volume\"] = [1] * len(time_window)\n",
    "        tid_direction_v_dummies = pd.get_dummies(tid_direction_df[\"vehicle_model\"], prefix=\"m\")\n",
    "        tid_direction_v_sep = pd.concat([tid_direction_df, tid_direction_v_dummies], axis = 1)\n",
    "        if key[1] == 0:\n",
    "            tid_direction_v_sep.drop([\"time\", \"vehicle_model\", \"vehicle_type\"], axis=1, inplace=True)\n",
    "        else:\n",
    "            tid_direction_v_sep.drop([\"time\", \"vehicle_model\"], axis=1, inplace=True)\n",
    "        data_grouped_by_timewindow = tid_direction_v_sep.groupby([\"time_window\"]).sum().reset_index()\n",
    "        data_grouped_by_timewindow.rename(columns={\"vehicle_type\": \"vehicle_type_1\"}, inplace=True)\n",
    "        missing_records_dict[\"time_window\"] = missing_timewindow\n",
    "        for column in data_grouped_by_timewindow.columns:\n",
    "            if column != \"time_window\":\n",
    "                missing_records_dict[column] = [0] * len(missing_timewindow)\n",
    "        missing_df = pd.DataFrame(missing_records_dict)\n",
    "        whole_tid_direction_data = data_grouped_by_timewindow.append(missing_df, ignore_index=True).sort([\"time_window\"])\n",
    "        tid_direction_dict[key] = whole_tid_direction_data\n",
    "        whole_tid_direction_data.to_csv(\"T\" + str(key[0]) + str(key[1]) + \"ti_\" + str(time_granularity) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_window(datetime):\n",
    "    ymd = datetime.isoformat(\" \").split(\" \")[0]\n",
    "    hour = time_wrapper(datetime.hour)\n",
    "    minute = time_wrapper(datetime.minute)\n",
    "    return ymd + \" \" + \":\".join([hour, minute])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_wrapper(num):\n",
    "    if num < 10:\n",
    "        return \"0\" + str(num)\n",
    "    else:\n",
    "        return str(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_min_mapping(time_granularity):\n",
    "    min_mapping = {}\n",
    "    i = 0\n",
    "    while (i+1) * time_granularity <= 60:\n",
    "        if (i+1) * time_granularity < 60:\n",
    "            min_mapping[time_wrapper(i*time_granularity)] = time_wrapper((i+1)*time_granularity)\n",
    "        else:\n",
    "            min_mapping[time_wrapper(i*time_granularity)] = \"00\"\n",
    "        i += 1\n",
    "    return min_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_remove_intervals(time_granularity):\n",
    "    remove_time_intervals_select = [[[0,0],[5,60-time_granularity]], \n",
    "                                    [[8,0],[14,60-time_granularity]], \n",
    "                                    [[17,0],[23,60-time_granularity]]]\n",
    "    remove_time_intervals_predict_ad = [[[0,0],[7,60-time_granularity]], \n",
    "                                        [[10,0],[16,60-time_granularity]], \n",
    "                                        [[19,0],[23,60-time_granularity]]]\n",
    "    remove_time_intervals_predict_am = [[[0,0],[7,60-time_granularity]], [[10,0],[23,60-time_granularity]]]\n",
    "    remove_time_intervals_predict_pm = [[[0,0],[16,60-time_granularity]], [[19,0],[23,60-time_granularity]]]\n",
    "    remove_time_intervals_irrelevant = [[[0,0],[5,60-time_granularity]], \n",
    "                                        [[10,0],[14,60-time_granularity]], \n",
    "                                        [[19,0],[23,60-time_granularity]]]\n",
    "    return [remove_time_intervals_select, remove_time_intervals_predict_ad, \n",
    "            remove_time_intervals_predict_am, remove_time_intervals_predict_pm,\n",
    "            remove_time_intervals_irrelevant]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(in_path):\n",
    "    data = pd.read_csv(in_path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_by_date(data, date_interval):\n",
    "    [date_low_bound, date_high_bound] = date_interval\n",
    "    time_window = data[\"time_window\"].tolist()\n",
    "    ymd_time_window = [x.split(\" \")[0] for x in time_window]\n",
    "    ymd_list_time_window = [map(int, x.split(\"-\")) for x in ymd_time_window]\n",
    "    datetime_list = [datetime(x[0], x[1], x[2]) for x in ymd_list_time_window]\n",
    "    low_datetime = datetime(date_low_bound[0], date_low_bound[1], date_low_bound[2])\n",
    "    high_datetime = datetime(date_high_bound[0], date_high_bound[1], date_high_bound[2])\n",
    "    remove_time_window = [True if x <= high_datetime and x >= low_datetime else False \n",
    "                          for x in datetime_list]\n",
    "    data[\"selection\"] = remove_time_window\n",
    "    no_national_day_data = data.loc[data[\"selection\"] == False]\n",
    "    return no_national_day_data.drop(\"selection\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_by_date_intervals(data, date_intervals):\n",
    "    for date_interval in date_intervals:\n",
    "        data = remove_by_date(data, date_interval)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_hm(hour, minute):\n",
    "    return datetime(1, 1, 1, hour, minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_by_time(data, time_interval):\n",
    "    [time_low_bound, time_high_bound] = time_interval\n",
    "    time_window = data[\"time_window\"].tolist()\n",
    "    hm_time_window = [x.split(\" \")[1] for x in time_window]\n",
    "    hm_list_time_window = [map(int, x.split(\":\")) for x in hm_time_window]\n",
    "    datetime_list = [datetime_hm(hour=x[0], minute=x[1]) for x in hm_list_time_window]\n",
    "    low_datetime = datetime_hm(hour=time_low_bound[0], minute=time_low_bound[1])\n",
    "    high_datetime = datetime_hm(hour=time_high_bound[0], minute=time_high_bound[1])\n",
    "    remove_time_window = [True if x <= high_datetime and x >= low_datetime else False \n",
    "                          for x in datetime_list]\n",
    "    data[\"selection\"] = remove_time_window\n",
    "    no_national_day_data = data.loc[data[\"selection\"] == False]\n",
    "    no_national_day_data.drop(\"selection\", 1, inplace=True)\n",
    "    return no_national_day_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_by_feature(data, feature_list):\n",
    "    return data[feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_by_time_intervals(data, time_intervals):\n",
    "    for time_interval in time_intervals:\n",
    "        data = remove_by_time(data, time_interval)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_time_window(row, time_interval):\n",
    "    time = row.split(\" \")\n",
    "    ymd = time[0]\n",
    "    hms = time[1]\n",
    "    m = time_wrapper(int(hms.split(\":\")[1]) / time_interval * time_interval)\n",
    "    return ymd + \" \" + hms.split(\":\")[0] + \":\" + m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_test_data(tollgate_id, direction, time_granularity):\n",
    "    data = pd.read_csv(\"volume_test1.csv\")\n",
    "    data_tid = data.loc[data[\"tollgate_id\"] == tollgate_id]\n",
    "    data_tid_direction = data_tid.loc[data_tid[\"direction\"] == direction]\n",
    "    tid_direction_v_dummies = pd.get_dummies(data_tid_direction[\"vehicle_model\"], prefix=\"m\")\n",
    "    data_tid_direction_v_sep = pd.concat([data_tid_direction, tid_direction_v_dummies], axis = 1)\n",
    "    data_tid_direction = data_tid_direction_v_sep.drop([\"vehicle_type\", \"tollgate_id\", \"direction\", \"vehicle_model\"], 1)\n",
    "    timedata = data_tid_direction[\"time\"].tolist()\n",
    "    time_window = map(functools.partial(transform_time_window, time_interval=time_granularity), timedata)\n",
    "    data_tid_direction[\"time_window\"] = time_window\n",
    "    data_tid_direction = data_tid_direction.drop([\"time\"], 1)\n",
    "    data_tid_direction[\"volume\"] = [1] * len(timedata)\n",
    "    data_grouped_by_timewindow = data_tid_direction.groupby([\"time_window\"]).sum().reset_index()\n",
    "    return data_grouped_by_timewindow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ind_ymd_mapping(data):\n",
    "    time_window = data[\"time_window\"].tolist()\n",
    "    seen = set()\n",
    "    ymd = [x.split(\" \")[0] for x in time_window]\n",
    "    distinct_ymd = [x for x in ymd if not (x in seen or seen.add(x))]\n",
    "    return dict(zip(range(len(distinct_ymd)), distinct_ymd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_by_hm(data, transform_list):\n",
    "    columns_hm_dict = {}\n",
    "    existent_columns = data.columns.tolist()\n",
    "    transform_columns = []\n",
    "    for column in transform_list:\n",
    "        if column in existent_columns and column != \"time_window\":\n",
    "            transform_columns.append(column)\n",
    "    time_window = data[\"time_window\"].tolist()\n",
    "    ymd = [x.split(\" \")[0] for x in time_window]\n",
    "    hm = [x.split(\" \")[1] for x in time_window]\n",
    "    hm_type = list(set(hm))\n",
    "    for column in transform_columns:\n",
    "        column_content = data[column].tolist()\n",
    "        for some_hm in hm_type:\n",
    "            columns_hm_dict[column + \"^\" + some_hm] = [column_content[i] \n",
    "                                                      for i in range(len(column_content)) \n",
    "                                                      if hm[i] == some_hm]\n",
    "    columns_hm_df = pd.DataFrame(columns_hm_dict)\n",
    "    return columns_hm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hour_plus(hour, plus):\n",
    "    next_hour = int(hour) + plus\n",
    "    return \"0\" + str(next_hour) if next_hour < 10 else str(next_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_datetime(hm_list, ind_ymd_dict, pred_interval, min_mapping, time_granularity):\n",
    "    if pred_interval == \"AM\":\n",
    "        hm_list = hm_list[:len(hm_list)/2]\n",
    "    if pred_interval == \"PM\":\n",
    "        hm_list = hm_list[len(hm_list)/2:]\n",
    "    time_window = []\n",
    "    left_min = [x.split(\":\")[1] for x in hm_list]\n",
    "    right_min = [min_mapping[x] for x in left_min]\n",
    "    left_hour = [hour_plus(x.split(\":\")[0], 2) for x in hm_list]\n",
    "    right_hour = [left_hour[i] if left_min[i] != str(60-time_granularity) else hour_plus(left_hour[i], 1) \n",
    "                  for i in range(len(left_hour))]\n",
    "    ymd_list = ind_ymd_dict.values()\n",
    "    for ymd in ymd_list:\n",
    "        ymd_datetime = [\"[\" + ymd + \" \" + left_hour[i] + \":\" + left_min[i] + \":\" + \"00\" + \",\" + \n",
    "                       ymd + \" \" + right_hour[i] + \":\" + right_min[i] + \":\" + \"00\" + \")\" for i in range(len(left_hour))]\n",
    "        time_window.extend(ymd_datetime)\n",
    "    return time_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prediction(select_df, predict_df, test_df, top_k):\n",
    "    select_df.to_csv(\"select_df.csv\",index=False)\n",
    "    predict_df.to_csv(\"predict_df.csv\",index=False)\n",
    "    test_df.to_csv(\"test_df.csv\",index=False)\n",
    "    predictions = {}\n",
    "    seen = set()\n",
    "    repe_columns = [x.split(\"^\")[0] for x in predict_df.columns.tolist()]\n",
    "    columns = [x for x in repe_columns if not (x in seen or seen.add(x))]\n",
    "    for column in columns:\n",
    "        predictions[column] = []\n",
    "    for i in range(test_df.shape[0]):\n",
    "        distance_list = [(j, dist(select_df.iloc[j].tolist(), test_df.iloc[i].tolist())) \n",
    "                         for j in range(select_df.shape[0])]   \n",
    "        sorted_distance_list = sorted(distance_list, key=operator.itemgetter(1))\n",
    "        top_k_sorted_distance_list = [x[0] for x in sorted_distance_list[:top_k]]\n",
    "        one_day_prediction = predict_df.iloc[top_k_sorted_distance_list].mean().tolist()\n",
    "        for ind, column in enumerate(columns):\n",
    "            predictions[column].extend(one_day_prediction[ind*len(one_day_prediction)/len(columns) : \n",
    "                                                          (ind+1)*len(one_day_prediction)/len(columns)])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist(a, b):\n",
    "    return np.mean([float(abs(a[i]-b[i]))/float(a[i] + 1) for i in range(len(a))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_an_interval(interval_str):\n",
    "    [start_date, end_date] = interval_str.split(\",\")\n",
    "    start_date_list = map(int, start_date.split(\"-\"))\n",
    "    end_date_list = map(int, end_date.split(\"-\"))\n",
    "    return [start_date_list, end_date_list]\n",
    "\n",
    "def get_date_intervals(date_intervals):\n",
    "    if \"\" in date_intervals: # no date is removed\n",
    "        return []\n",
    "    else:\n",
    "        return map(get_an_interval, date_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(input_path, remove_dates, train_test_proportion, time_granularity):\n",
    "    total_intervals = 24 * 60 / time_granularity\n",
    "    data = read_data(input_path)\n",
    "    data_removing_dates = remove_by_date_intervals(data.copy(deep=True), remove_dates)\n",
    "#     print int(data_removing_dates.shape[0]/total_intervals * train_test_proportion)\n",
    "    return data_removing_dates, int(data_removing_dates.shape[0]/total_intervals * train_test_proportion) * total_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMSP_alpha(data, tollgate_id, direction, k, train_test_split_pos, pred_interval, \n",
    "               min_mapping, remove_time_intervals, time_granularity, selection_list,\n",
    "               prediction_list, feature_list):\n",
    "    \"\"\"\n",
    "    remove_time_intervals: 0 for select, 1 for ad, 2 for am, 3 for pm, 4 for irrelevant\n",
    "    \"\"\"\n",
    "    data = select_by_feature(data.copy(True), feature_list)\n",
    "    train_data = data.iloc[:train_test_split_pos]\n",
    "    test_data = data.iloc[train_test_split_pos:]    \n",
    "    features = [x for x in prediction_list if x not in [\"time_window\"]]    \n",
    "    \n",
    "    train_data_for_tree = remove_by_time_intervals(train_data.copy(deep=True), remove_time_intervals[4])\n",
    "    \n",
    "    rt_y = train_data_for_tree[\"volume\"]\n",
    "    rt_X = train_data_for_tree[features]\n",
    "    rt = DTR(min_impurity_split = 1)\n",
    "#     rt = DTR(min_samples_split=5)\n",
    "    rt.fit(rt_X, rt_y)\n",
    "    \n",
    "    train_data_selection = remove_by_time_intervals(train_data.copy(deep=True), remove_time_intervals[0])\n",
    "    if pred_interval == \"AM\":\n",
    "        remove_time_intervals_predict = remove_time_intervals[2]\n",
    "    elif pred_interval == \"PM\":\n",
    "        remove_time_intervals_predict = remove_time_intervals[3]\n",
    "    else:\n",
    "        remove_time_intervals_predict = remove_time_intervals[1]\n",
    "    train_data_prediction = remove_by_time_intervals(train_data.copy(deep=True), remove_time_intervals_predict)\n",
    "    test_data_feature = remove_by_time_intervals(test_data.copy(deep=True), remove_time_intervals[0])\n",
    "    test_data_response = remove_by_time_intervals(test_data.copy(deep=True), remove_time_intervals_predict)\n",
    "    \n",
    "    test_data_ind_ymd = ind_ymd_mapping(test_data_response)\n",
    "    data_ind_ymd = ind_ymd_mapping(train_data_selection.copy(True))\n",
    "    \n",
    "    selection_columns_by_hm = columns_by_hm(train_data_selection.copy(True), selection_list)\n",
    "    prediction_columns_by_hm = columns_by_hm(train_data_prediction.copy(True), prediction_list)\n",
    "    test_columns_by_hm = columns_by_hm(test_data_feature, selection_list)\n",
    "    \n",
    "    column_list = test_columns_by_hm.columns.tolist()\n",
    "    selected_column_list = column_list[:len(column_list)/(len(selection_list)-1)]\n",
    "    hm_list = [x.split(\"^\")[1] for x in selected_column_list]\n",
    "    \n",
    "    output_info = {}\n",
    "    output_info[\"time_window\"] = format_datetime(hm_list, test_data_ind_ymd, pred_interval, min_mapping, time_granularity)\n",
    "    prediction_df = pd.DataFrame(get_prediction(selection_columns_by_hm, prediction_columns_by_hm, test_columns_by_hm, k))\n",
    "    output_info[\"tollgate_id\"] = [tollgate_id] * len(output_info[\"time_window\"])\n",
    "    output_info[\"direction\"] = [direction] * len(output_info[\"time_window\"])\n",
    "    output_df = pd.concat([pd.DataFrame(output_info), prediction_df], axis=1)\n",
    "    rt_pred_real = rt.predict(test_data_response[features]).tolist()\n",
    "    rt_pred_fake = rt.predict(prediction_df[features]).tolist()\n",
    "    truth = test_data_response[\"volume\"].tolist()\n",
    "    mape_fake = np.mean([float(abs(rt_pred_fake[i] - truth[i]))/(truth[i]) for i in range(len(truth))])\n",
    "    mape_real = np.mean([float(abs(rt_pred_real[i] - truth[i]))/(truth[i]) for i in range(len(truth))])\n",
    "    return output_df, mape_fake, mape_real, rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_k_mape(input_file, pred_interval, time_granularity):\n",
    "    input_params = open(input_file, \"r\")\n",
    "    input_lines = input_params.readlines()\n",
    "    input_tables_info = [x.split(\"\\n\")[0].split(\":\")[0].split(\" \") for x in input_lines]\n",
    "    remove_date_intervals = [x.split(\"\\n\")[0].split(\":\")[1].split(\" \") for x in input_lines]\n",
    "    remove_dates_list = map(get_date_intervals, remove_date_intervals)\n",
    "    min_mapping = get_min_mapping(time_granularity)\n",
    "    feature_list = [\"time_window\", \"m_1\", \"m_2\", \"m_3\", \"has_etc\", \"volume\"]\n",
    "    selection_list = [\"time_window\", \"m_1\", \"m_2\", \"m_3\", \"has_etc\"]\n",
    "    prediction_list = [\"time_window\", \"volume\"]\n",
    "    remove_time_intervals = get_remove_intervals(time_granularity)\n",
    "    best_k_pred_mape_rt = []\n",
    "    dataframes = []\n",
    "    for i in range(len(input_tables_info)):\n",
    "        k_pred_mape_rt = []\n",
    "        data, train_test_split_pos = data_preprocess(input_tables_info[i][0], remove_dates_list[i], 0.75, time_granularity)\n",
    "        print \"%s \\n\" % input_tables_info[i][0]\n",
    "        for k in range(1, train_test_split_pos/(24 * 60 / time_granularity) + 1):\n",
    "            df, error_fake, error_real, rt = KMSP_alpha(data, int(input_tables_info[i][1]), int(input_tables_info[i][2]), \n",
    "                                   k, train_test_split_pos, pred_interval, min_mapping, remove_time_intervals, \n",
    "                                   time_granularity, selection_list, prediction_list, feature_list)\n",
    "            print \"k = %d, fake MAPE = %f, real MAPE = %f \\n\" % (k, error_fake, error_real)\n",
    "            k_pred_mape_rt.append((k, df, error_fake, rt))\n",
    "        best_k_pred_mape_rt.append(sorted(k_pred_mape_rt, key=operator.itemgetter(2))[0])\n",
    "    print \"Best k-MAPE pairs for all tollgate-direction combinations:\"\n",
    "    for i in range(len(input_tables_info)):\n",
    "        print input_tables_info[i][0] + \": \" + str((best_k_pred_mape_rt[i][0], best_k_pred_mape_rt[i][2])) + \"\\n\"\n",
    "        dataframes.append(best_k_pred_mape_rt[i][1])\n",
    "    print \"Final MAPE: %f \\n\" % np.mean([x[2] for x in best_k_pred_mape_rt])\n",
    "    final_output = pd.concat(dataframes)\n",
    "#     final_output = final_output[[\"tollgate_id\", \"time_window\", \"direction\", \"volume\"]]\n",
    "    final_output.to_csv(\"T2_submission_test.csv\", index=False)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T10ti_20.csv \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xzou\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\ipykernel\\__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\xzou\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\ipykernel\\__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1, fake MAPE = 0.229803, real MAPE = 0.017523 \n",
      "\n",
      "k = 2, fake MAPE = 0.192480, real MAPE = 0.017523 \n",
      "\n",
      "k = 3, fake MAPE = 0.180343, real MAPE = 0.017523 \n",
      "\n",
      "k = 4, fake MAPE = 0.180241, real MAPE = 0.017523 \n",
      "\n",
      "k = 5, fake MAPE = 0.166010, real MAPE = 0.017523 \n",
      "\n",
      "k = 6, fake MAPE = 0.170200, real MAPE = 0.017523 \n",
      "\n",
      "k = 7, fake MAPE = 0.177078, real MAPE = 0.017523 \n",
      "\n",
      "k = 8, fake MAPE = 0.178830, real MAPE = 0.017523 \n",
      "\n",
      "k = 9, fake MAPE = 0.174899, real MAPE = 0.017523 \n",
      "\n",
      "k = 10, fake MAPE = 0.173464, real MAPE = 0.017523 \n",
      "\n",
      "k = 11, fake MAPE = 0.176772, real MAPE = 0.017523 \n",
      "\n",
      "k = 12, fake MAPE = 0.177395, real MAPE = 0.017523 \n",
      "\n",
      "k = 13, fake MAPE = 0.181957, real MAPE = 0.017523 \n",
      "\n",
      "k = 14, fake MAPE = 0.183743, real MAPE = 0.017523 \n",
      "\n",
      "T11ti_20.csv \n",
      "\n",
      "k = 1, fake MAPE = 0.181268, real MAPE = 0.009329 \n",
      "\n",
      "k = 2, fake MAPE = 0.176603, real MAPE = 0.009329 \n",
      "\n",
      "k = 3, fake MAPE = 0.167997, real MAPE = 0.009329 \n",
      "\n",
      "k = 4, fake MAPE = 0.163997, real MAPE = 0.009329 \n",
      "\n",
      "k = 5, fake MAPE = 0.156553, real MAPE = 0.009329 \n",
      "\n",
      "k = 6, fake MAPE = 0.158354, real MAPE = 0.009329 \n",
      "\n",
      "k = 7, fake MAPE = 0.153056, real MAPE = 0.009329 \n",
      "\n",
      "k = 8, fake MAPE = 0.153143, real MAPE = 0.009329 \n",
      "\n",
      "k = 9, fake MAPE = 0.153114, real MAPE = 0.009329 \n",
      "\n",
      "k = 10, fake MAPE = 0.160986, real MAPE = 0.009329 \n",
      "\n",
      "k = 11, fake MAPE = 0.157466, real MAPE = 0.009329 \n",
      "\n",
      "k = 12, fake MAPE = 0.156261, real MAPE = 0.009329 \n",
      "\n",
      "k = 13, fake MAPE = 0.154879, real MAPE = 0.009329 \n",
      "\n",
      "k = 14, fake MAPE = 0.158958, real MAPE = 0.009329 \n",
      "\n",
      "k = 15, fake MAPE = 0.157630, real MAPE = 0.009329 \n",
      "\n",
      "k = 16, fake MAPE = 0.158781, real MAPE = 0.009329 \n",
      "\n",
      "T20ti_20.csv \n",
      "\n",
      "k = 1, fake MAPE = 0.172939, real MAPE = 0.008891 \n",
      "\n",
      "k = 2, fake MAPE = 0.141289, real MAPE = 0.008891 \n",
      "\n",
      "k = 3, fake MAPE = 0.116134, real MAPE = 0.008891 \n",
      "\n",
      "k = 4, fake MAPE = 0.121731, real MAPE = 0.008891 \n",
      "\n",
      "k = 5, fake MAPE = 0.124214, real MAPE = 0.008891 \n",
      "\n",
      "k = 6, fake MAPE = 0.123346, real MAPE = 0.008891 \n",
      "\n",
      "k = 7, fake MAPE = 0.124538, real MAPE = 0.008891 \n",
      "\n",
      "k = 8, fake MAPE = 0.132910, real MAPE = 0.008891 \n",
      "\n",
      "k = 9, fake MAPE = 0.130849, real MAPE = 0.008891 \n",
      "\n",
      "k = 10, fake MAPE = 0.138503, real MAPE = 0.008891 \n",
      "\n",
      "k = 11, fake MAPE = 0.141162, real MAPE = 0.008891 \n",
      "\n",
      "k = 12, fake MAPE = 0.141850, real MAPE = 0.008891 \n",
      "\n",
      "k = 13, fake MAPE = 0.142932, real MAPE = 0.008891 \n",
      "\n",
      "k = 14, fake MAPE = 0.143860, real MAPE = 0.008891 \n",
      "\n",
      "k = 15, fake MAPE = 0.142259, real MAPE = 0.008891 \n",
      "\n",
      "T30ti_20.csv \n",
      "\n",
      "k = 1, fake MAPE = 0.117378, real MAPE = 0.006705 \n",
      "\n",
      "k = 2, fake MAPE = 0.130183, real MAPE = 0.006705 \n",
      "\n",
      "k = 3, fake MAPE = 0.138841, real MAPE = 0.006705 \n",
      "\n",
      "k = 4, fake MAPE = 0.131613, real MAPE = 0.006705 \n",
      "\n",
      "k = 5, fake MAPE = 0.131377, real MAPE = 0.006705 \n",
      "\n",
      "k = 6, fake MAPE = 0.127912, real MAPE = 0.006705 \n",
      "\n",
      "k = 7, fake MAPE = 0.126204, real MAPE = 0.006705 \n",
      "\n",
      "k = 8, fake MAPE = 0.120256, real MAPE = 0.006705 \n",
      "\n",
      "k = 9, fake MAPE = 0.123814, real MAPE = 0.006705 \n",
      "\n",
      "k = 10, fake MAPE = 0.127387, real MAPE = 0.006705 \n",
      "\n",
      "k = 11, fake MAPE = 0.125665, real MAPE = 0.006705 \n",
      "\n",
      "k = 12, fake MAPE = 0.125155, real MAPE = 0.006705 \n",
      "\n",
      "k = 13, fake MAPE = 0.126447, real MAPE = 0.006705 \n",
      "\n",
      "k = 14, fake MAPE = 0.126614, real MAPE = 0.006705 \n",
      "\n",
      "k = 15, fake MAPE = 0.132303, real MAPE = 0.006705 \n",
      "\n",
      "k = 16, fake MAPE = 0.133343, real MAPE = 0.006705 \n",
      "\n",
      "k = 17, fake MAPE = 0.136874, real MAPE = 0.006705 \n",
      "\n",
      "k = 18, fake MAPE = 0.136985, real MAPE = 0.006705 \n",
      "\n",
      "k = 19, fake MAPE = 0.135412, real MAPE = 0.006705 \n",
      "\n",
      "k = 20, fake MAPE = 0.138648, real MAPE = 0.006705 \n",
      "\n",
      "k = 21, fake MAPE = 0.137474, real MAPE = 0.006705 \n",
      "\n",
      "T31ti_20.csv \n",
      "\n",
      "k = 1, fake MAPE = 0.230352, real MAPE = 0.006368 \n",
      "\n",
      "k = 2, fake MAPE = 0.180099, real MAPE = 0.006368 \n",
      "\n",
      "k = 3, fake MAPE = 0.167430, real MAPE = 0.006368 \n",
      "\n",
      "k = 4, fake MAPE = 0.168097, real MAPE = 0.006368 \n",
      "\n",
      "k = 5, fake MAPE = 0.179266, real MAPE = 0.006368 \n",
      "\n",
      "k = 6, fake MAPE = 0.176997, real MAPE = 0.006368 \n",
      "\n",
      "k = 7, fake MAPE = 0.170907, real MAPE = 0.006368 \n",
      "\n",
      "k = 8, fake MAPE = 0.170233, real MAPE = 0.006368 \n",
      "\n",
      "k = 9, fake MAPE = 0.167333, real MAPE = 0.006368 \n",
      "\n",
      "k = 10, fake MAPE = 0.166099, real MAPE = 0.006368 \n",
      "\n",
      "k = 11, fake MAPE = 0.164305, real MAPE = 0.006368 \n",
      "\n",
      "k = 12, fake MAPE = 0.163391, real MAPE = 0.006368 \n",
      "\n",
      "k = 13, fake MAPE = 0.164275, real MAPE = 0.006368 \n",
      "\n",
      "k = 14, fake MAPE = 0.162417, real MAPE = 0.006368 \n",
      "\n",
      "k = 15, fake MAPE = 0.163304, real MAPE = 0.006368 \n",
      "\n",
      "Best k-MAPE pairs for all tollgate-direction combinations:\n",
      "T10ti_20.csv: (5, 0.16601037202014271)\n",
      "\n",
      "T11ti_20.csv: (7, 0.15305647233068168)\n",
      "\n",
      "T20ti_20.csv: (3, 0.1161335934999701)\n",
      "\n",
      "T30ti_20.csv: (1, 0.11737844923803403)\n",
      "\n",
      "T31ti_20.csv: (14, 0.16241705014103094)\n",
      "\n",
      "Final MAPE: 0.142999 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>direction</th>\n",
       "      <th>time_window</th>\n",
       "      <th>tollgate_id</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-13 08:00:00,2016-10-13 08:20:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>51.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-13 08:20:00,2016-10-13 08:40:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>51.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-13 08:40:00,2016-10-13 09:00:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>52.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-13 09:00:00,2016-10-13 09:20:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>49.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-13 09:20:00,2016-10-13 09:40:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>56.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-13 09:40:00,2016-10-13 10:00:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>50.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-13 17:00:00,2016-10-13 17:20:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>44.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-13 17:20:00,2016-10-13 17:40:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>44.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-13 17:40:00,2016-10-13 18:00:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-13 18:00:00,2016-10-13 18:20:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>29.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-13 18:20:00,2016-10-13 18:40:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>28.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-13 18:40:00,2016-10-13 19:00:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>23.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-14 08:00:00,2016-10-14 08:20:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>49.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-14 08:20:00,2016-10-14 08:40:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>55.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-14 08:40:00,2016-10-14 09:00:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>56.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-14 09:00:00,2016-10-14 09:20:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>49.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-14 09:20:00,2016-10-14 09:40:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>58.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-14 09:40:00,2016-10-14 10:00:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>52.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-14 17:00:00,2016-10-14 17:20:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>47.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-14 17:20:00,2016-10-14 17:40:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>48.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-14 17:40:00,2016-10-14 18:00:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>38.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-14 18:00:00,2016-10-14 18:20:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-14 18:20:00,2016-10-14 18:40:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>30.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-14 18:40:00,2016-10-14 19:00:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>28.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-15 08:00:00,2016-10-15 08:20:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>46.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-15 08:20:00,2016-10-15 08:40:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>54.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-15 08:40:00,2016-10-15 09:00:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>55.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-15 09:00:00,2016-10-15 09:20:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>48.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-15 09:20:00,2016-10-15 09:40:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>55.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>[2016-10-15 09:40:00,2016-10-15 10:00:00)</td>\n",
       "      <td>1</td>\n",
       "      <td>49.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-15 17:00:00,2016-10-15 17:20:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>85.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-15 17:20:00,2016-10-15 17:40:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>89.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-15 17:40:00,2016-10-15 18:00:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>94.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-15 18:00:00,2016-10-15 18:20:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>69.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-15 18:20:00,2016-10-15 18:40:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>71.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-15 18:40:00,2016-10-15 19:00:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>72.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-16 08:00:00,2016-10-16 08:20:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>116.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-16 08:20:00,2016-10-16 08:40:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>123.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-16 08:40:00,2016-10-16 09:00:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>135.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-16 09:00:00,2016-10-16 09:20:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>124.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-16 09:20:00,2016-10-16 09:40:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>117.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-16 09:40:00,2016-10-16 10:00:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>100.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-16 17:00:00,2016-10-16 17:20:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>85.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-16 17:20:00,2016-10-16 17:40:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>91.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-16 17:40:00,2016-10-16 18:00:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>96.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-16 18:00:00,2016-10-16 18:20:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>70.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-16 18:20:00,2016-10-16 18:40:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>70.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-16 18:40:00,2016-10-16 19:00:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>70.785714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-17 08:00:00,2016-10-17 08:20:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>117.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-17 08:20:00,2016-10-17 08:40:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>125.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-17 08:40:00,2016-10-17 09:00:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>137.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-17 09:00:00,2016-10-17 09:20:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>126.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-17 09:20:00,2016-10-17 09:40:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>119.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-17 09:40:00,2016-10-17 10:00:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>101.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-17 17:00:00,2016-10-17 17:20:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>86.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-17 17:20:00,2016-10-17 17:40:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>90.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-17 17:40:00,2016-10-17 18:00:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>96.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-17 18:00:00,2016-10-17 18:20:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>69.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-17 18:20:00,2016-10-17 18:40:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>70.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1</td>\n",
       "      <td>[2016-10-17 18:40:00,2016-10-17 19:00:00)</td>\n",
       "      <td>3</td>\n",
       "      <td>71.357143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>372 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    direction                                time_window  tollgate_id  \\\n",
       "0           0  [2016-10-13 08:00:00,2016-10-13 08:20:00)            1   \n",
       "1           0  [2016-10-13 08:20:00,2016-10-13 08:40:00)            1   \n",
       "2           0  [2016-10-13 08:40:00,2016-10-13 09:00:00)            1   \n",
       "3           0  [2016-10-13 09:00:00,2016-10-13 09:20:00)            1   \n",
       "4           0  [2016-10-13 09:20:00,2016-10-13 09:40:00)            1   \n",
       "5           0  [2016-10-13 09:40:00,2016-10-13 10:00:00)            1   \n",
       "6           0  [2016-10-13 17:00:00,2016-10-13 17:20:00)            1   \n",
       "7           0  [2016-10-13 17:20:00,2016-10-13 17:40:00)            1   \n",
       "8           0  [2016-10-13 17:40:00,2016-10-13 18:00:00)            1   \n",
       "9           0  [2016-10-13 18:00:00,2016-10-13 18:20:00)            1   \n",
       "10          0  [2016-10-13 18:20:00,2016-10-13 18:40:00)            1   \n",
       "11          0  [2016-10-13 18:40:00,2016-10-13 19:00:00)            1   \n",
       "12          0  [2016-10-14 08:00:00,2016-10-14 08:20:00)            1   \n",
       "13          0  [2016-10-14 08:20:00,2016-10-14 08:40:00)            1   \n",
       "14          0  [2016-10-14 08:40:00,2016-10-14 09:00:00)            1   \n",
       "15          0  [2016-10-14 09:00:00,2016-10-14 09:20:00)            1   \n",
       "16          0  [2016-10-14 09:20:00,2016-10-14 09:40:00)            1   \n",
       "17          0  [2016-10-14 09:40:00,2016-10-14 10:00:00)            1   \n",
       "18          0  [2016-10-14 17:00:00,2016-10-14 17:20:00)            1   \n",
       "19          0  [2016-10-14 17:20:00,2016-10-14 17:40:00)            1   \n",
       "20          0  [2016-10-14 17:40:00,2016-10-14 18:00:00)            1   \n",
       "21          0  [2016-10-14 18:00:00,2016-10-14 18:20:00)            1   \n",
       "22          0  [2016-10-14 18:20:00,2016-10-14 18:40:00)            1   \n",
       "23          0  [2016-10-14 18:40:00,2016-10-14 19:00:00)            1   \n",
       "24          0  [2016-10-15 08:00:00,2016-10-15 08:20:00)            1   \n",
       "25          0  [2016-10-15 08:20:00,2016-10-15 08:40:00)            1   \n",
       "26          0  [2016-10-15 08:40:00,2016-10-15 09:00:00)            1   \n",
       "27          0  [2016-10-15 09:00:00,2016-10-15 09:20:00)            1   \n",
       "28          0  [2016-10-15 09:20:00,2016-10-15 09:40:00)            1   \n",
       "29          0  [2016-10-15 09:40:00,2016-10-15 10:00:00)            1   \n",
       "..        ...                                        ...          ...   \n",
       "42          1  [2016-10-15 17:00:00,2016-10-15 17:20:00)            3   \n",
       "43          1  [2016-10-15 17:20:00,2016-10-15 17:40:00)            3   \n",
       "44          1  [2016-10-15 17:40:00,2016-10-15 18:00:00)            3   \n",
       "45          1  [2016-10-15 18:00:00,2016-10-15 18:20:00)            3   \n",
       "46          1  [2016-10-15 18:20:00,2016-10-15 18:40:00)            3   \n",
       "47          1  [2016-10-15 18:40:00,2016-10-15 19:00:00)            3   \n",
       "48          1  [2016-10-16 08:00:00,2016-10-16 08:20:00)            3   \n",
       "49          1  [2016-10-16 08:20:00,2016-10-16 08:40:00)            3   \n",
       "50          1  [2016-10-16 08:40:00,2016-10-16 09:00:00)            3   \n",
       "51          1  [2016-10-16 09:00:00,2016-10-16 09:20:00)            3   \n",
       "52          1  [2016-10-16 09:20:00,2016-10-16 09:40:00)            3   \n",
       "53          1  [2016-10-16 09:40:00,2016-10-16 10:00:00)            3   \n",
       "54          1  [2016-10-16 17:00:00,2016-10-16 17:20:00)            3   \n",
       "55          1  [2016-10-16 17:20:00,2016-10-16 17:40:00)            3   \n",
       "56          1  [2016-10-16 17:40:00,2016-10-16 18:00:00)            3   \n",
       "57          1  [2016-10-16 18:00:00,2016-10-16 18:20:00)            3   \n",
       "58          1  [2016-10-16 18:20:00,2016-10-16 18:40:00)            3   \n",
       "59          1  [2016-10-16 18:40:00,2016-10-16 19:00:00)            3   \n",
       "60          1  [2016-10-17 08:00:00,2016-10-17 08:20:00)            3   \n",
       "61          1  [2016-10-17 08:20:00,2016-10-17 08:40:00)            3   \n",
       "62          1  [2016-10-17 08:40:00,2016-10-17 09:00:00)            3   \n",
       "63          1  [2016-10-17 09:00:00,2016-10-17 09:20:00)            3   \n",
       "64          1  [2016-10-17 09:20:00,2016-10-17 09:40:00)            3   \n",
       "65          1  [2016-10-17 09:40:00,2016-10-17 10:00:00)            3   \n",
       "66          1  [2016-10-17 17:00:00,2016-10-17 17:20:00)            3   \n",
       "67          1  [2016-10-17 17:20:00,2016-10-17 17:40:00)            3   \n",
       "68          1  [2016-10-17 17:40:00,2016-10-17 18:00:00)            3   \n",
       "69          1  [2016-10-17 18:00:00,2016-10-17 18:20:00)            3   \n",
       "70          1  [2016-10-17 18:20:00,2016-10-17 18:40:00)            3   \n",
       "71          1  [2016-10-17 18:40:00,2016-10-17 19:00:00)            3   \n",
       "\n",
       "        volume  \n",
       "0    51.600000  \n",
       "1    51.800000  \n",
       "2    52.200000  \n",
       "3    49.800000  \n",
       "4    56.200000  \n",
       "5    50.400000  \n",
       "6    44.600000  \n",
       "7    44.200000  \n",
       "8    35.000000  \n",
       "9    29.800000  \n",
       "10   28.400000  \n",
       "11   23.200000  \n",
       "12   49.200000  \n",
       "13   55.000000  \n",
       "14   56.000000  \n",
       "15   49.800000  \n",
       "16   58.600000  \n",
       "17   52.400000  \n",
       "18   47.600000  \n",
       "19   48.400000  \n",
       "20   38.200000  \n",
       "21   32.000000  \n",
       "22   30.600000  \n",
       "23   28.600000  \n",
       "24   46.600000  \n",
       "25   54.600000  \n",
       "26   55.200000  \n",
       "27   48.400000  \n",
       "28   55.200000  \n",
       "29   49.600000  \n",
       "..         ...  \n",
       "42   85.142857  \n",
       "43   89.714286  \n",
       "44   94.785714  \n",
       "45   69.428571  \n",
       "46   71.000000  \n",
       "47   72.571429  \n",
       "48  116.357143  \n",
       "49  123.928571  \n",
       "50  135.285714  \n",
       "51  124.785714  \n",
       "52  117.500000  \n",
       "53  100.285714  \n",
       "54   85.785714  \n",
       "55   91.071429  \n",
       "56   96.357143  \n",
       "57   70.500000  \n",
       "58   70.785714  \n",
       "59   70.785714  \n",
       "60  117.500000  \n",
       "61  125.357143  \n",
       "62  137.928571  \n",
       "63  126.428571  \n",
       "64  119.285714  \n",
       "65  101.642857  \n",
       "66   86.571429  \n",
       "67   90.857143  \n",
       "68   96.071429  \n",
       "69   69.857143  \n",
       "70   70.142857  \n",
       "71   71.357143  \n",
       "\n",
       "[372 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_k_mape(\"input_file_for_look.txt\", \"AD\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def KMSP_release(input_path, tollgate_id, direction, k, remove_dates, pred_interval, time_granularity, \n",
    "                remove_time_intervals, selection_list, prediction_list, feature_list, min_mapping):\n",
    "    data = read_data(input_path)\n",
    "    data_removing_dates = remove_by_date_intervals(data.copy(deep=True), remove_dates)\n",
    "    data_selection = remove_by_time_intervals(data_removing_dates.copy(deep=True), remove_time_intervals[0])\n",
    "    \n",
    "    features = [x for x in prediction_list if x not in [\"time_window\"]]    \n",
    "    \n",
    "    train_data_for_tree = remove_by_time_intervals(data_removing_dates.copy(deep=True), remove_time_intervals[4])\n",
    "    \n",
    "    rt_y = train_data_for_tree[\"volume\"]\n",
    "    rt_X = train_data_for_tree[features]\n",
    "    rt = DTR(min_samples_split=5)\n",
    "    rt.fit(rt_X, rt_y)\n",
    "    \n",
    "    if pred_interval == \"AM\":\n",
    "        remove_time_intervals_predict = remove_time_intervals[2]\n",
    "    elif pred_interval == \"PM\":\n",
    "        remove_time_intervals_predict = remove_time_intervals[3]\n",
    "    else:\n",
    "        remove_time_intervals_predict = remove_time_intervals[1]\n",
    "    data_prediction = remove_by_time_intervals(data_removing_dates.copy(deep=True), remove_time_intervals_predict)\n",
    "    data_ind_ymd = ind_ymd_mapping(data_selection.copy(True))\n",
    "    selection_columns_by_hm = columns_by_hm(data_selection.copy(True), selection_list)\n",
    "    prediction_columns_by_hm = columns_by_hm(data_prediction.copy(True), prediction_list)\n",
    "    test_data = make_test_data(tollgate_id, direction, time_granularity)\n",
    "    test_data.to_csv(\"test_data.csv\", index=False)\n",
    "    test_data_ind_ymd = ind_ymd_mapping(test_data)\n",
    "    test_columns_by_hm = columns_by_hm(test_data.copy(True), selection_list)\n",
    "    column_list = test_columns_by_hm.columns.tolist()\n",
    "    selected_column_list = column_list[:len(column_list)/(len(selection_list)-1)]\n",
    "    hm_list = [x.split(\"^\")[1] for x in selected_column_list]\n",
    "    \n",
    "    output_info = {}\n",
    "    output_info[\"time_window\"] = format_datetime(hm_list, test_data_ind_ymd, pred_interval, min_mapping, time_granularity)\n",
    "    prediction_df = pd.DataFrame(get_prediction(selection_columns_by_hm, prediction_columns_by_hm, test_columns_by_hm, k))\n",
    "    prediction_df.to_csv(\"for_look.csv\", index=False)\n",
    "    rt_pred = rt.predict(prediction_df[features]).tolist()\n",
    "    output_info[\"volume\"] = map(int, map(round, rt_pred))\n",
    "    output_info[\"tollgate_id\"] = [tollgate_id] * len(output_info[\"time_window\"])\n",
    "    output_info[\"direction\"] = [direction] * len(output_info[\"time_window\"])\n",
    "    output_df = pd.DataFrame(output_info)\n",
    "    return output_df, data_ind_ymd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_submission(input_file, time_granularity):\n",
    "    input_params = open(input_file, \"r\")\n",
    "    input_lines = input_params.readlines()\n",
    "    input_tables_info = [x.split(\"\\n\")[0].split(\":\")[0].split(\" \") for x in input_lines]\n",
    "    remove_date_intervals = [x.split(\"\\n\")[0].split(\":\")[1].split(\" \") for x in input_lines]\n",
    "    k_list = [x.split(\"\\n\")[0].split(\":\")[2].split(\" \") for x in input_lines]\n",
    "    remove_dates_list = map(get_date_intervals, remove_date_intervals)\n",
    "    remove_time_intervals = get_remove_intervals(time_granularity)\n",
    "    min_mapping = get_min_mapping(time_granularity)\n",
    "    feature_list = [\"time_window\", \"m_1\", \"m_2\", \"m_3\", \"has_etc\", \"volume\"]\n",
    "    selection_list = [\"time_window\", \"m_1\", \"m_2\", \"m_3\", \"has_etc\"]\n",
    "    prediction_list = [\"time_window\", \"volume\"]\n",
    "    dataframes = []\n",
    "    for ind, time_interval in enumerate([\"AD\"]):\n",
    "        for i in range(len(input_tables_info)):\n",
    "            df, data_ind_ymd = KMSP_release(input_tables_info[i][0], \n",
    "                                            int(input_tables_info[i][1]), \n",
    "                                            int(input_tables_info[i][2]), \n",
    "                                            int(k_list[i][ind]), \n",
    "                                            remove_dates_list[i], \n",
    "                                            time_interval, time_granularity, remove_time_intervals,\n",
    "                                            selection_list, prediction_list, \n",
    "                                            feature_list, min_mapping)\n",
    "            dataframes.append(df)\n",
    "    final_output = pd.concat(dataframes)\n",
    "    final_output = final_output[[\"tollgate_id\", \"time_window\", \"direction\", \"volume\"]]\n",
    "    final_output.to_csv(\"T2_submission_test.csv\", index=False)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['5'], ['7'], ['3'], ['1'], ['14']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xzou\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\ipykernel\\__main__.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\xzou\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\ipykernel\\__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "final_result = get_submission(\"input_file.txt\", 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
